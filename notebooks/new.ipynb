{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/cyril-k/projects/llm-colosseum/.venv/lib/python3.12/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/cyril-k/projects/llm-colosseum/.venv/lib/python3.12/site-packages/certifi/cacert.pem'\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Hey there', 'role': 'user'}], 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.studio.nebius.ai/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.studio.nebius.ai' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x168e2a5a0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1692c0dd0> server_hostname='api.studio.nebius.ai' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x168e29f70>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 21 Oct 2024 19:25:35 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'532'), (b'Connection', b'keep-alive'), (b'x-ratelimit-limit-requests', b'600.0'), (b'x-ratelimit-limit-tokens', b'400000.0'), (b'x-ratelimit-remaining-requests', b'600.0'), (b'x-ratelimit-remaining-tokens', b'400000.0'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains')])\n",
      "INFO:httpx:HTTP Request: POST https://api.studio.nebius.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.studio.nebius.ai/v1/chat/completions \"200 OK\" Headers({'date': 'Mon, 21 Oct 2024 19:25:35 GMT', 'content-type': 'application/json', 'content-length': '532', 'connection': 'keep-alive', 'x-ratelimit-limit-requests': '600.0', 'x-ratelimit-limit-tokens': '400000.0', 'x-ratelimit-remaining-requests': '600.0', 'x-ratelimit-remaining-tokens': '400000.0', 'strict-transport-security': 'max-age=31536000; includeSubDomains'})\n",
      "DEBUG:openai._base_client:request_id: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, raw=None, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from llama_index.llms.langchain import LangChainLLM\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "api_base=\"https://api.studio.nebius.ai/v1\"\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "client = LangChainLLM(\n",
    "    llm=ChatOpenAI(\n",
    "        base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "        api_key=os.environ[\"NB_AI_STUDIO_KEY\"],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "\n",
    "client.complete(prompt=\"Hey there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"recvmsg:Connection reset by peer\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"recvmsg:Connection reset by peer\", grpc_status:14, created_time:\"2024-10-21T23:41:44.751941+02:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdiambra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dclient \u001b[38;5;241m=\u001b[39m diambra\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mClient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m195.242.16.143:50051\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiambra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmpty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/llm-colosseum/.venv/lib/python3.12/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1175\u001b[0m     (\n\u001b[1;32m   1176\u001b[0m         state,\n\u001b[1;32m   1177\u001b[0m         call,\n\u001b[1;32m   1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m     )\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/llm-colosseum/.venv/lib/python3.12/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"recvmsg:Connection reset by peer\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"recvmsg:Connection reset by peer\", grpc_status:14, created_time:\"2024-10-21T23:41:44.751941+02:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "import diambra.engine\n",
    "\n",
    "dclient = diambra.engine.Client(\"195.242.16.143:50051\")\n",
    "dclient.Step(diambra.engine.model.Empty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
